---
title: "Part 2: NASA Crater Detection"
excerpt: "Mid-project update: crater detection on the Moon using YOLOv10 and Ellipse R-CNN"
date: 2025-03-28
layout: single
author_profile: true
read_time: true
share: true
related: true
classes: wide
categories:
  - deep-learning 
tags:
  - computer vision
  - object detection
  - deep learning
---

_This is Part 2 of a two-part series on crater detection using deep learning. [Missed Part 1? Catch up here →](https://anekha.github.io/deep-learning/2025/03/28/NASA.html)_

## 🚀 Overview

In Part 1, we built a **YOLOv10 + Ellipse R-CNN** pipeline to detect and localize lunar craters.

This post focuses on:
- 🌀 Rim-fitting using Ellipse R-CNN  
- 📊 Evaluation metrics and precision challenges  
- ⚙️ System integration and resource constraints  
- 🔍 Final takeaways and next steps

---

📺 **Watch a 1-minute summary of the project:**

<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; border-radius: 8px; margin-bottom: 1.5rem;">
  <iframe 
    src="https://www.youtube.com/embed/5sAfHHuAaLE?si=Jqw0OIvaw-Hfthlq" 
    title="NASA Crater Detection Summary"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    allowfullscreen
    style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border-radius: 8px;">
  </iframe>
</div>

---


## 🧠 From Boxes to Rims: LunarLens Pipeline

Our two-stage detection system — **LunarLens** — works like this:

1. **YOLOv10** detects crater bounding boxes quickly.  
2. **Ellipse R-CNN** then fits an ellipse to the rim of each crater.

This hybrid system balances speed with shape accuracy, ideal for autonomous space navigation.

![LunarLens Pipeline](/assets/projects/NASA/lunarlens_pipeline.png){: .center-image }

---

## 🌀 Postprocessing with Ellipse R-CNN

Bounding boxes from YOLOv10 are cropped and passed to Ellipse R-CNN, which predicts:

- (x, y) center  
- Major & minor axes  
- Orientation  

This gives us a mathematically accurate rim outline.

```text
Crater 1: (100.3, 40.8), (101.5, 39.6), ...
Crater 2: (463.2, 80.9), (462.3, 81.4), ...
```

## 📊 Evaluation & Metrics

We assessed detection quality at **IoU thresholds of 50% and 70%**, using AP, precision, and recall.

| Metric    | IoU = 50 | IoU = 70 |
|-----------|----------|----------|
| AP        | 0.111    | 0.003    |
| Recall    | 0.145    | 0.010    |
| Precision | 0.337    | 0.042    |

🧠 **Key takeaway**: Precision drops sharply at high IoU — highlighting how tight rim localization is the hard part, not detection.

## 🧪 System Performance Under Constraints

We were tasked with running this system on **CPU-only hardware** with Raspberry Pi–level specs.

| Constraint   | Result           |
|--------------|------------------|
| Image Size   | 5 MP             |
| Runtime      | ~20 sec / image  |
| RAM          | < 4 GB           |
| Hardware     | No GPU           |

---

## 🔍 Final Thoughts & Next Steps

- ✅ **YOLOv10** was great for rapid detection, but rim precision required postprocessing.  
- ✅ **Ellipse R-CNN** helped convert detections into usable geometric inputs.  
- ⚠️ Evaluation at high IoU exposed the need for better shape-fit training.

**Next steps:**
- Fine-tune Ellipse R-CNN on lunar tile crops  
- Apply weighted evaluation based on crater size  
- Improve YOLO thresholding and ensemble strategies  

---

## 🤝 Acknowledgments

Built by: **Anekha Sokhal, Tian Le, Henry Tran, Juan Hevia, Madeleine Harrell, Jeremy Xu**  
Mentored by: **Kyle Smith (NASA), Arko Barman, Ananya Muguli**

---

💬 _Have questions, or want to explore vision systems for autonomous spacecraft?_  
**Feel free to reach out — I’d love to chat!**